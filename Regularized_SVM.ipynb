{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "## Regularized Support Vector Machine\n",
        "### Joseph Melby\n",
        "### 11/02/18\n",
        "### Problem 1\n",
        "#### Given training data: MNIST X train.csv (feature values), MNIST y train.csv (labels)\n",
        "\n",
        "#### Test data: MNIST X test.csv (feature values), MNIST y test.csv (labels) .\n",
        "\n",
        "#### File House feature MNIST description.csv gives a brief introduction to these data sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label  [2]\n",
            "[-1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n",
            "[-1. -1. -1. -1. -1. -1. -1. -1. -1.  1.]\n",
            "[-1. -1.  1. -1. -1. -1. -1. -1. -1. -1.]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
            "  warnings.warn(msg, DataConversionWarning)\n",
            "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:90: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/ipykernel/__main__.py:90: RuntimeWarning: invalid value encountered in multiply\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9aaa534aae57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m \u001b[0mweights_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_reg_OVR_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_ohe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0mplot_digit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9aaa534aae57>\u001b[0m in \u001b[0;36mlog_reg_OVR_train\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 10 columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0my_train_one_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# pick ith columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mweights_one_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_reg_binary_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_one_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_one_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9aaa534aae57>\u001b[0m in \u001b[0;36mlog_reg_binary_train\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mcoeffs_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mb_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mcoeffs_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeffs_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcoeffs_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9aaa534aae57>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(X, y, c, b, reg, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mhingegrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhinge_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mloss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormgrad\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhingegrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# updating coeffs upon the gradient change]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9aaa534aae57>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(X, y_train, c, b, reg)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mhingeloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mhingeloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhingeloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2332\u001b[0m     \"\"\"\n\u001b[1;32m   2333\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out, keepdims=keepdims,\n\u001b[0;32m-> 2334\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   2335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline \n",
        "\n",
        "def read_dataset(feature_file, label_file):\n",
        "    ''' Read data set in *.csv to data frame in Pandas'''\n",
        "    df_X = pd.read_csv(feature_file)\n",
        "    df_y = pd.read_csv(label_file)\n",
        "    X = df_X.values # convert values in dataframe to numpy array (features)\n",
        "    y = df_y.values # convert values in dataframe to numpy array (label)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "X_train, y_train = read_dataset('MNIST_X_train.csv', 'MNIST_y_train.csv')\n",
        "X_test, y_test = read_dataset('MNIST_X_test.csv', 'MNIST_y_test.csv')\n",
        "\n",
        "#print(X_train.shape)\n",
        "#print(X_test.shape)\n",
        "\n",
        "def plot_digit(feature_vector): \n",
        "    plt.gray() \n",
        "    plt.matshow(feature_vector.reshape(8,8))\n",
        "    plt.show() \n",
        "\n",
        "#plot_digit(X_train[0])\n",
        "print('Label ', y_train[0])\n",
        "\n",
        "def normalize_features(X_train, X_test):\n",
        "    from sklearn.preprocessing import StandardScaler #import libaray\n",
        "    scaler = StandardScaler() # call an object function\n",
        "    scaler.fit(X_train) # calculate mean, std in X_train\n",
        "    X_train_norm = scaler.transform(X_train) # apply normalization on X_train\n",
        "    X_test_norm = scaler.transform(X_test) # we use the same normalization on X_test\n",
        "    return X_train_norm, X_test_norm\n",
        "\n",
        "X_train_norm, X_test_norm = normalize_features(X_train, X_test)\n",
        "\n",
        "\n",
        "def one_hot_encoder(y_train, y_test):\n",
        "    trainneg = np.ones(shape=(len(y_train),10))\n",
        "    train = -1*trainneg\n",
        "    testneg = np.ones(shape=(len(y_train),10))\n",
        "    test = -1*testneg\n",
        "    for i in range(len(y_train)):\n",
        "        trainval = y_train[i]\n",
        "        train[i,trainval] = 1\n",
        "        testval = y_train[i]\n",
        "        test[i,testval] = 1\n",
        "        y_train_ohe = train\n",
        "        y_test_ohe = test\n",
        "    return y_train_ohe, y_test_ohe\n",
        "# label is 0 -> [1 0 0 0 0 0 0 0 0]\n",
        "# label is 3 -> [0 0 0 1 0 0 0 0 0]\n",
        "\n",
        "y_train_ohe, y_test_ohe = one_hot_encoder(y_train, y_test)\n",
        "print(y_train_ohe[0])\n",
        "print(y_train_ohe[1])\n",
        "print(y_test_ohe[0])\n",
        "\n",
        "\n",
        "# Predictor function should be a general hyperplane equation.\n",
        "def predictor(X, c, b):\n",
        "    ##hyperplane function\n",
        "     return X.dot(c)+b\n",
        "\n",
        "\n",
        "######################### Our classifier at the end should output either 1 or -1 ##################################\n",
        "    \n",
        "def loss(X, y_train, c, b, reg):\n",
        "#     print(y_pred.shape)\n",
        "    hingeloss = np.zeros(shape=(len(y_train)))\n",
        "    for i in range(len(y_train)):\n",
        "        hingeloss[i]= np.max([0, 1-y_train[i]*predictor(X,c,b)[i]])\n",
        "    return np.linalg.norm(c) + reg*sum(hingeloss)\n",
        "\n",
        "def hinge_gradient(X, y_train, c, b):\n",
        "    hingegrad = np.zeros(shape=(len(c)+1))\n",
        "    for i in range(len(y_train)):\n",
        "        if 1-y_train[i]*predictor(X,c,b)[i] > 0:\n",
        "            hingegrad[0] += -y_train[i]\n",
        "            for j in range(len(c)):\n",
        "                hingegrad[j+1] = -y_train[i]*X[i,j]\n",
        "    return hingegrad\n",
        "\n",
        "def gradient_descent(X, y, c, b, reg, epochs=1000, learning_rate=0.01):\n",
        "    loss_history = [0]*epochs\n",
        "    normgrad = np.zeros(shape=(len(c)+1))\n",
        "    for i in range(len(c)):\n",
        "        normgrad[i+1] = c[i]*(np.linalg.norm(c))**(-1/2)\n",
        "    hingegrad = hinge_gradient(X, y_train, c, b)\n",
        "    for epoch in range(epochs):\n",
        "        loss_history[epoch] = loss(X, y, c, b, reg).ravel()\n",
        "        gradient = normgrad + hingegrad\n",
        "        # updating coeffs upon the gradient change]\n",
        "        b = b - learning_rate*gradient[0]\n",
        "        c = c - learning_rate*gradient[1:]\n",
        "    return c, b, loss_history\n",
        "\n",
        "\n",
        "def log_reg_binary_train(X_train, y_train):  \n",
        "    ''' Training our model based on the training data\n",
        "        Input: X_train: input features\n",
        "               y_train: binary labels\n",
        "        Return: coeffs of the logistic model\n",
        "    '''\n",
        "    coeffs_0 = np.zeros((X_train_norm.shape[1], 1))\n",
        "    b_0 = 0\n",
        "    coeffs_grad, b_grad, history_loss = gradient_descent(X_train, y_train, coeffs_0, b_0, 100, epochs=100, learning_rate=0.1)\n",
        "    return coeffs_grad\n",
        "\n",
        "\n",
        "def log_reg_OVR_train(X_train, y_train):# y_train: one_hot_encoder labels\n",
        "    # y_train will have 10 columns\n",
        "    weights = []\n",
        "    for i in range(y_train.shape[1]): # 10 columns \n",
        "        y_train_one_column = y_train[:,i] # pick ith columns\n",
        "        weights_one_column = log_reg_binary_train(X_train, y_train_one_column)\n",
        "        weights.append(weights_one_column)\n",
        "    return weights\n",
        "\n",
        "\n",
        "def prediction(weights_list, X_test):\n",
        "    i = 0\n",
        "    for weights in weights_list:\n",
        "        decision_one_column = predictor(X_test, weights)\n",
        "        # probabily of one column\n",
        "        if i == 0:\n",
        "            decision_matrix = decision_one_column\n",
        "        else:\n",
        "            # combine all decision columns to form a matrix\n",
        "            decision_matrix = np.concatenate(\n",
        "                              (decision_matrix, decision_one_column),\n",
        "                               axis=1)\n",
        "        i += 1\n",
        "    labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "    num_test_samples = X_test.shape[0]\n",
        "    # find which index gives us the highest probability\n",
        "    ypred = np.zeros(num_test_samples, dtype=int) \n",
        "    for i in range(num_test_samples):\n",
        "        ypred[i] = labels[np.argmax(decision_matrix[i,:])]\n",
        "    return ypred\n",
        "\n",
        "\n",
        "weights_list = log_reg_OVR_train(X_train_norm, y_train_ohe)\n",
        "index = 20    \n",
        "plot_digit(X_test[index])\n",
        "ypred = prediction(weights_list, X_test_norm[index:index+1])\n",
        "print(ypred)\n",
        "\n",
        "def accuracy(ypred, yexact):\n",
        "    p = np.array(ypred == yexact, dtype = int)\n",
        "    return np.sum(p)/float(len(yexact))\n",
        "\n",
        "ypred = prediction(weights_list, X_test_norm)\n",
        "print('Accuracy of our model ', accuracy(ypred, y_test.ravel()))\n",
        "\n",
        "\n",
        "# Stochastic GD (SGD)\n",
        "def SGD(X, y, c, epochs=1000, learning_rate=0.00, batch_size=10):\n",
        "    y = y.reshape(-1, 1)\n",
        "    loss_history = [0]*epochs\n",
        "    for epoch in range(epochs):\n",
        "        # loop through batches\n",
        "        batch_loss = []\n",
        "        for i in np.arange(0, X.shape[0], batch_size):\n",
        "            X_current_batch = X[i:i+batch_size]\n",
        "            y_current_batch = y[i:i+batch_size]\n",
        "            yhat = predictor(X_current_batch, c)\n",
        "            loss_current_batch = loss_function(X_current_batch, c, y_current_batch).ravel()\n",
        "            batch_loss.append(loss_current_batch)\n",
        "            gradient = XT.dot(yhat - y)/float(len(y))\n",
        "            c = c - learning_rate*gradient\n",
        "        loss_history[epoch] = np.average(batch_loss)\n",
        "    return c, loss_history"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "bc0da0bd7c01680fa8348b7a26d6a24694e547ab16f98204be81d32b3a15c474"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
